import os
import sys
from dotenv import load_dotenv

# utils ì„í¬íŠ¸ìš© ê²½ë¡œ ì¶”ê°€
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from utils.spark_session import create_spark_session
from spark.utils.readers import read_from_kafka
from utils.text_cleaner import clean_job_details
from spark.utils.writers import write_to_s3, write_to_postgres

load_dotenv() 

def run_spark_job():
    print("ğŸš€ Spark Job Started: Ingestion & Processing")

    # 1. Spark ì„¸ì…˜ ìƒì„±
    spark = create_spark_session("MentoAI_Main_Pipeline")

    # 2. Kafkaë¡œë¶€í„° ë°ì´í„° ì½ê¸°
    raw_df = read_from_kafka(spark)

    # 3. ë°ì´í„° íŒŒì‹± ë° ì •ì œ (JSON -> Schema -> Clean)
    refined_df = clean_job_details(raw_df)
    
    # ë””ë²„ê¹…ìš©: ìŠ¤í‚¤ë§ˆ ì¶œë ¥
    refined_df.printSchema()

    # 4. ê²°ê³¼ ì €ì¥ (Multi-Sink: S3 & Postgres)
    query_s3 = write_to_s3(refined_df)
    # query_db = write_to_postgres(refined_df)
    
    # 5. ìŠ¤íŠ¸ë¦¬ë° ì¢…ë£Œ ëŒ€ê¸°
    query_s3.awaitTermination()
    # query_db.awaitTermination()
    
    print("âœ… All Streaming Jobs Completed!")

if __name__ == "__main__":
    run_spark_job()