import os
from dotenv import load_dotenv
from pyspark.sql import SparkSession
from pyspark.sql.functions import from_json, col
from pyspark.sql.types import StructType, StructField, StringType

load_dotenv()

def create_spark_session():

    aws_access_key = os.getenv("AWS_ACCESS_KEY_ID")
    aws_secret_key = os.getenv("AWS_SECRET_ACCESS_KEY")
    aws_region = os.getenv("AWS_REGION", "ap-northeast-2")

    if not aws_access_key or not aws_secret_key:
        raise ValueError("âŒ .env íŒŒì¼ì— AWS Access Keyê°€ ì—†ìŠµë‹ˆë‹¤!")

    # Spark ì„¸ì…˜ ìƒì„± (AWS S3 íŒ¨í‚¤ì§€ í¬í•¨)
    spark = SparkSession.builder \
        .appName("MentoAI_Career_Ingestion") \
        .master("local[*]") \
        .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.262") \
        .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem") \
        .config("spark.hadoop.fs.s3a.access.key", aws_access_key) \
        .config("spark.hadoop.fs.s3a.secret.key", aws_secret_key) \
        .config("spark.hadoop.fs.s3a.endpoint", f"s3.{aws_region}.amazonaws.com") \
        .getOrCreate()
    return spark

def run_spark_job():
    spark = create_spark_session()
    spark.sparkContext.setLogLevel("WARN")

    bucket_name = os.getenv("S3_BUCKET_NAME")
    print(f"ğŸš€ Spark Streaming ì‹œì‘: Kafka -> AWS S3 ({bucket_name})")

    kafka_df = spark.readStream \
        .format("kafka") \
        .option("kafka.bootstrap.servers", "localhost:9092") \
        .option("subscribe", "career_raw") \
        .option("startingOffsets", "earliest") \
        .load()

    # JSON ë°ì´í„° ìŠ¤í‚¤ë§ˆ ì •ì˜ 
    schema = StructType([
        StructField("source_id", StringType()),
        StructField("company", StringType()),
        StructField("title", StringType()),
        StructField("link", StringType()),
        StructField("pay", StringType()),
        StructField("location", StringType()),
        StructField("deadline", StringType()),
        StructField("reg_date", StringType()),
        StructField("description", StringType()),
        StructField("requirements", StringType()),
        StructField("preferred_qualifications", StringType()),
        StructField("collected_at", StringType())
    ])

    # ë°ì´í„° íŒŒì‹±
    processed_df = kafka_df.selectExpr("CAST(value AS STRING)") \
        .select(from_json(col("value"), schema).alias("data")) \
        .select("data.*")

    # S3ì— ì €ì¥ (Parquet í¬ë§·)
    # path: ì‹¤ì œ ë°ì´í„°ê°€ ì €ì¥ë  ê²½ë¡œ
    # checkpointLocation: ìŠ¤íŠ¸ë¦¬ë° ìƒíƒœ ì €ì¥
    query = processed_df.writeStream \
        .format("parquet") \
        .outputMode("append") \
        .option("path", f"s3a://{bucket_name}/raw/") \
        .option("checkpointLocation", f"s3a://{bucket_name}/checkpoints/") \
        .start()

    print("â³ AWS S3ë¡œ ë°ì´í„° ì ì¬ ì¤‘... (Ctrl+Cë¡œ ì¢…ë£Œ)")
    query.awaitTermination()

if __name__ == "__main__":
    run_spark_job()