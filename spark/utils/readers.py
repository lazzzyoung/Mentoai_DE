import os
from pyspark.sql import DataFrame

# Kafkaë¡œë¶€í„° ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ë°ì´í„°ë¥¼ ì½ì–´ì˜µë‹ˆë‹¤.
def read_stream_from_kafka(spark, bootstrap_servers, topic_name, starting_offsets="earliest"):
    """
    [Job 1ìš©] Kafkaë¡œë¶€í„° ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ë°ì´í„°ë¥¼ ì½ì–´ì˜µë‹ˆë‹¤.
    """
    print(f"ğŸ“¡ Kafka Read Stream ì´ˆê¸°í™”: {bootstrap_servers} | Topic: {topic_name}")
    return spark.readStream \
        .format("kafka") \
        .option("kafka.bootstrap.servers", bootstrap_servers) \
        .option("subscribe", topic_name) \
        .option("startingOffsets", starting_offsets) \
        .option("failOnDataLoss", "false") \
        .load()

# S3 Bronze Layer(Parquet)ì— ìƒˆë¡œ ì¶”ê°€ë˜ëŠ” íŒŒì¼ì„ ê°ì‹œí•˜ë©° ì½ì–´ì˜µë‹ˆë‹¤.
def read_stream_from_s3(spark, bucket_name, source_path):
    
    full_path = f"s3a://{bucket_name}/{source_path}"
    print(f"ğŸ“‚ S3 Stream Read ì‹œì‘: {full_path}")
    
    # ì €ì¥ëœ Parquet íŒŒì¼ì—ì„œ ìŠ¤í‚¤ë§ˆë¥¼ ìƒ˜í”Œë§.
    try:
        sample_schema = spark.read.parquet(full_path).schema
    except Exception:
        # ë°ì´í„°ê°€ ì•„ì˜ˆ ì—†ì„ ê²½ìš°ë¥¼ ëŒ€ë¹„í•œ ê¸°ë³¸ ìŠ¤í‚¤ë§ˆ (ìµœì´ˆ ì‹¤í–‰ ì‹œ í•„ìš”)
        from pyspark.sql.types import StructType, StructField, StringType
        sample_schema = StructType([StructField("raw_json", StringType())])

    return spark.readStream \
        .format("parquet") \
        .schema(sample_schema) \
        .option("maxFilesPerTrigger", 100) \
        .load(full_path)