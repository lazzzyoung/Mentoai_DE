import sys
import os
from dotenv import load_dotenv
from pyspark.sql.functions import col

# ê²½ë¡œ ì„¤ì •
sys.path.append(os.path.dirname(os.path.abspath(os.path.dirname(__file__))))
from utils.spark_session import create_spark_session
from utils.text_cleaner import clean_job_details


load_dotenv()

def run_recovery_silver():
    # ì„¸ì…˜ ìƒì„±
    spark = create_spark_session("MentoAI_Job2_Silver_Recovery")
    
    bucket_name = os.getenv("S3_BUCKET_NAME")
    bronze_path = f"s3a://{bucket_name}/bronze/career_raw/"
    
    print(f"ğŸ“‚ Reading ALL data from S3 Bronze (Batch Mode): {bronze_path}")

    # read (Batch)
    try:
        
        raw_df = spark.read.parquet(bronze_path)
    except Exception as e:
        print(f"âš ï¸ ì—ëŸ¬ ë°œìƒ: {e}")
        return

    # ë°ì´í„° ì •ì œ
    input_df = raw_df.withColumnRenamed("raw_json", "value")
    refined_df = clean_job_details(input_df)
    
    count = refined_df.count()
    print(f" ì •ì œëœ ë°ì´í„° ê°œìˆ˜: {count}ê±´")

    if count > 0:
        # PostgreSQLì— ì§ì ‘ ì“°ê¸° (Batch)
        db_url = os.getenv("DB_URL", "jdbc:postgresql://postgres:5432/mentoai")
        print("ğŸ’¾ Saving to Postgres...")
        
        refined_df.write \
            .format("jdbc") \
            .option("url", db_url) \
            .option("dbtable", "career_jobs") \
            .option("user", "airflow") \
            .option("password", "airflow") \
            .option("driver", "org.postgresql.Driver") \
            .mode("overwrite") \
            .save()
        
        print("ğŸ‰ career_jobs í…Œì´ë¸” ìƒì„± ë° ë°ì´í„° ì ì¬ ì™„ë£Œ!")
    else:
        print(" ì ì¬í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

    spark.stop()

if __name__ == "__main__":
    run_recovery_silver()