import sys
import os
from dotenv import load_dotenv
from pyspark.sql.functions import col

# ê²½ë¡œ ì„¤ì •
sys.path.append(os.path.dirname(os.path.abspath(os.path.dirname(__file__))))
from utils.spark_session import create_spark_session
from utils.text_cleaner import clean_job_details
from utils.writers import write_to_postgres

load_dotenv()

def run_process_silver():
    spark = create_spark_session("MentoAI_Job2_Silver")
    
    bucket_name = os.getenv("S3_BUCKET_NAME")
    
    bronze_path = f"s3a://{bucket_name}/bronze/career_raw/"
    
    print(f"ğŸ“‚ Reading from S3 Bronze: {bronze_path}")

    # S3 Bronze ë°ì´í„° ì½ê¸°
    try:
        bronze_schema = spark.read.parquet(bronze_path).schema
    except Exception as e:
        print("âš ï¸ Bronze ë°ì´í„° ê²½ë¡œê°€ ì—†ê±°ë‚˜ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. ë¨¼ì € job_ingest_bronze.pyë¥¼ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
        return

    # Streaming DataFrame ìƒì„±
    raw_file_df = spark.readStream \
        .format("parquet") \
        .schema(bronze_schema) \
        .option("maxFilesPerTrigger", 100) \
        .load(bronze_path)
    
    # Bronzeì˜ 'raw_json' ì»¬ëŸ¼ì„ 'value'ë¡œ ë³€ê²½í•˜ì—¬ íŒ¨ìŠ¤
    input_df = raw_file_df.withColumnRenamed("raw_json", "value")
    
    # íŒŒì‹± ë° ì •ì œ
    refined_df = clean_job_details(input_df)
    
    # PostgreSQL Silverì— ì €ì¥
    query = write_to_postgres(refined_df)
    
    print("â³ Silver Layer(Postgres) ì ì¬ ì¤‘...")
    query.awaitTermination()

if __name__ == "__main__":
    run_process_silver()