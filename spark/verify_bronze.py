import sys
import os
from dotenv import load_dotenv

sys.path.append(os.path.dirname(os.path.abspath(os.path.dirname(__file__))))
from utils.spark_session import create_spark_session
from utils.readers import read_stream_from_kafka
from utils.writers import write_raw_to_s3 

load_dotenv()


def verify_bronze_data():
    spark = create_spark_session("Verify_Bronze_Data")
    
    bucket_name = os.getenv("S3_BUCKET_NAME")
    # Bronze ë°ì´í„° ê²½ë¡œ
    bronze_path = f"s3a://{bucket_name}/bronze/career_raw/"

    print(f"ğŸ” S3 ê²½ë¡œ í™•ì¸: {bronze_path}")

    try:
        
        df = spark.read.parquet(bronze_path)
        
        count = df.count()
        print(f"ğŸ“Š ì´ ì €ì¥ëœ ë°ì´í„° ê°œìˆ˜: {count}ê±´")
        
        if count > 0:
            print("\nğŸ“‹ [ìƒ˜í”Œ ë°ì´í„° Top 3]")
            df.selectExpr(
                "collected_date", 
                "raw_json as json_preview", 
                "ingestion_time"
            ).show(3, truncate=False)
        else:
            print("âš ï¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. Producerë¥¼ ì‹¤í–‰í–ˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
            
    except Exception as e:
        print(f"âŒ ì½ê¸° ì‹¤íŒ¨ (ê²½ë¡œê°€ ì—†ê±°ë‚˜ ê¶Œí•œ ë¬¸ì œì¼ ìˆ˜ ìˆìŒ): {e}")

if __name__ == "__main__":
    verify_bronze_data()