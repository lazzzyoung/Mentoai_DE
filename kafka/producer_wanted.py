import os
import sys
import json
import time
import logging
import requests
from dotenv import load_dotenv
from kafka import KafkaProducer
from kafka.errors import KafkaError
load_dotenv() 

sys.path.append(os.path.dirname(os.path.abspath(os.path.dirname(__file__))))

from utils.wanted_scraper import fetch_job_id_list, fetch_job_detail_raw


# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("WantedProducer")

BASE_URL = os.getenv('WANTED_BASE_URL')
GROUP_ID = os.getenv('TARGET_JOB_GROUP') # ê°œë°œ
JOB_ID_CODE = os.getenv('TARGET_JOB_ID') # ë°ì´í„° ì—”ì§€ë‹ˆì–´
BOOTSTRAP_SERVERS = os.getenv('KAFKA_BOOTSTRAP_SERVERS', 'localhost:9092')
TOPIC_NAME = os.getenv('KAFKA_TOPIC_NAME', 'career_raw')

def run_producer():
    logger.info("ğŸ¬ Wanted Producer ì‹œì‘...")
    
    try:
        producer = KafkaProducer(
            bootstrap_servers=[BOOTSTRAP_SERVERS],
            value_serializer=lambda v: json.dumps(v).encode('utf-8'),
            key_serializer=lambda v: str(v).encode('utf-8'),
            retries=3  
        )
        logger.info(f"Kafka Connected: {BOOTSTRAP_SERVERS}")
    except Exception as e:
        logger.critical(f"Kafka ì—°ê²° ì‹¤íŒ¨! Error: {e}")
        return

   
    with requests.Session() as session:
        
        session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Referer': BASE_URL
        })

        # ID ë¦¬ìŠ¤íŠ¸ ìˆ˜ì§‘
        logger.info("ê³µê³  ID ë¦¬ìŠ¤íŠ¸ ìˆ˜ì§‘ ì¤‘...")
        target_ids = fetch_job_id_list(session, BASE_URL, GROUP_ID, JOB_ID_CODE, limit=50)
        
        if not target_ids:
            logger.warning("âš ï¸ ìˆ˜ì§‘ëœ ê³µê³  IDê°€ ì—†ìŠµë‹ˆë‹¤. ì¢…ë£Œí•©ë‹ˆë‹¤.")
            producer.close()
            return

        logger.info(f"ğŸ‘‰ ì´ {len(target_ids)}ê°œì˜ ê³µê³  ìƒì„¸ ì •ë³´ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.")

        # ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ ë° Kafka ì „ì†¡
        success_count = 0
        fail_count = 0

        for job_id in target_ids:
            try:
               
                raw_data = fetch_job_detail_raw(session, BASE_URL, job_id)
                
                if not raw_data:
                    fail_count += 1
                    continue 

                message = {
                    "source": "wanted",
                    "source_id": str(job_id),
                    "collected_at": time.strftime('%Y-%m-%dT%H:%M:%SZ'),
                    "raw_data": raw_data 
                }

                producer.send(
                    TOPIC_NAME,
                    key=str(job_id), # Log Compactionì„ ìœ„í•œ Key ì„¤ì •
                    value=message
                )
                
                success_count += 1
                if success_count % 10 == 0:
                    logger.info(f"   ... {success_count}ê±´ ì „ì†¡ ì™„ë£Œ")
                
                time.sleep(0.5)

            except KafkaError as ke:
                logger.error(f"âš ï¸ Kafka ì „ì†¡ ì—ëŸ¬ (ID: {job_id}): {ke}")
                fail_count += 1
            except Exception as e:
                logger.error(f"âš ï¸ ì•Œ ìˆ˜ ì—†ëŠ” ì—ëŸ¬ (ID: {job_id}): {e}")
                fail_count += 1
                continue 

  
        producer.flush() 
        producer.close()
        logger.info(f"ì‘ì—… ì™„ë£Œ! ì„±ê³µ: {success_count}, ì‹¤íŒ¨: {fail_count}")

if __name__ == "__main__":
    run_producer()