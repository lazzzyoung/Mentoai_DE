# kafka/producer.py
import time
import json
import re
import requests
import random
from bs4 import BeautifulSoup
from kafka import KafkaProducer

def get_kafka_producer():
    return KafkaProducer(
        bootstrap_servers=['localhost:9092'],
        value_serializer=lambda v: json.dumps(v).encode('utf-8')
    )

def clean_space(text):
    if not text: return ""
    return re.sub(r'\s+', ' ', text).strip()

def get_detail_info(auth_no):
    """
    [í•„ë“œ ë¶„ë¦¬ ê¸°ëŠ¥ ì¶”ê°€]
    ë‹¨ìˆœ í…ìŠ¤íŠ¸ ë¤í”„ê°€ ì•„ë‹ˆë¼, 'ìê²©ìš”ê±´', 'ìš°ëŒ€ì‚¬í•­', 'ì§ë¬´ë‚´ìš©'ì„ ë¶„ë¦¬í•´ì„œ ì¶”ì¶œ
    """
    detail_url = f"https://www.work.go.kr/empInfo/empInfoSrch/detail/empDetailAuthView.do?wantedAuthNo={auth_no}"
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) Chrome/120.0.0.0 Safari/537.36'
    }
    
    print(f"   ğŸ‘‰ [Detail] ìƒì„¸ ì ‘ì†: {auth_no}")
    
    try:
        time.sleep(random.uniform(0.3, 0.6))
        
        resp = requests.get(detail_url, headers=headers)
        if resp.status_code != 200:
            print(f"   âŒ [Detail] ì ‘ì† ì‹¤íŒ¨ (Status: {resp.status_code})")
            return None

        soup = BeautifulSoup(resp.text, 'html.parser')
        
        # -------------------------------------------------------
        # ë°ì´í„° êµ¬ì¡°í™” ë¡œì§
        # -------------------------------------------------------
        # ê¸°ë³¸ê°’ ì„¤ì •
        extracted_data = {
            "job_description": "ìƒì„¸ ë‚´ìš© ì—†ìŒ", # ì§ë¬´ë‚´ìš©
            "requirements": "í•™ë ¥/ê²½ë ¥ ë¬´ê´€",    # ìê²©ìš”ê±´ (ê²½ë ¥, í•™ë ¥)
            "preferred": "ìš°ëŒ€ì‚¬í•­ ì—†ìŒ"         # ìš°ëŒ€ì‚¬í•­ (ì „ê³µ, ìê²©ì¦, ìš°ëŒ€ì¡°ê±´)
        }
        
        content_area = soup.find(id='contents') or soup.find(class_='emp_detail')
        
        if content_area:
            # 1. ì§ë¬´ë‚´ìš© ì¶”ì¶œ (ë³´í†µ 'ì§ë¬´ë‚´ìš©' í—¤ë”ë¥¼ ê°€ì§„ iframeì´ë‚˜ tdì— ìˆìŒ)
            # ì›Œí¬ë„·ì€ ì§ë¬´ë‚´ìš©ì´ í…ìŠ¤íŠ¸ë¡œ ê¸¸ê²Œ ë“¤ì–´ê°€ë¯€ë¡œ ë³„ë„ íƒìƒ‰
            job_desc_header = content_area.find(lambda tag: tag.name == "th" and "ì§ë¬´ë‚´ìš©" in tag.get_text())
            if job_desc_header:
                # th ë°”ë¡œ ë‹¤ìŒì˜ tdë¥¼ ì°¾ìŒ
                job_desc_body = job_desc_header.find_next_sibling('td')
                if job_desc_body:
                    extracted_data["job_description"] = clean_space(job_desc_body.get_text())

            # 2. ìê²©ìš”ê±´ & ìš°ëŒ€ì‚¬í•­ (í…Œì´ë¸” ì „ì²´ ìŠ¤ìº”)
            # 'th' íƒœê·¸ì˜ í…ìŠ¤íŠ¸ë¥¼ ë³´ê³  íŒë‹¨í•˜ì—¬ 'td'ì˜ ë‚´ìš©ì„ ìˆ˜ì§‘
            
            req_list = []
            pref_list = []
            
            tables = content_area.find_all('table')
            for table in tables:
                rows = table.find_all('tr')
                for row in rows:
                    th = row.find('th')
                    td = row.find('td')
                    if not th or not td: continue
                    
                    header_text = clean_space(th.get_text())
                    body_text = clean_space(td.get_text())
                    
                    if not body_text: continue

                    # (A) ìê²©ìš”ê±´ ê´€ë ¨ í‚¤ì›Œë“œ
                    if any(kw in header_text for kw in ["ê²½ë ¥ì¡°ê±´", "í•™ë ¥", "ëª¨ì§‘ì§ì¢…"]):
                        req_list.append(f"{header_text}: {body_text}")
                        
                    # (B) ìš°ëŒ€ì‚¬í•­ ê´€ë ¨ í‚¤ì›Œë“œ
                    elif any(kw in header_text for kw in ["ìš°ëŒ€ì¡°ê±´", "ì „ê³µ", "ìê²©ë©´í—ˆ", "ì™¸êµ­ì–´", "ì»´í“¨í„°"]):
                        # 'ë¹„í¬ë§', 'ê´€ê³„ì—†ìŒ' ê°™ì€ ì˜ë¯¸ ì—†ëŠ” ë°ì´í„°ëŠ” ì œì™¸
                        if "ë¹„í¬ë§" not in body_text and "ê´€ê³„ì—†ìŒ" not in body_text:
                            pref_list.append(f"{header_text}: {body_text}")

            # ë¦¬ìŠ¤íŠ¸ë¥¼ ë¬¸ìì—´ë¡œ í•©ì¹˜ê¸°
            if req_list:
                extracted_data["requirements"] = " | ".join(req_list)
            
            if pref_list:
                extracted_data["preferred"] = " | ".join(pref_list)
            
            print(f"      âœ… ì§ë¬´: {len(extracted_data['job_description'])}ì | ìê²©: {bool(req_list)} | ìš°ëŒ€: {bool(pref_list)}")

        else:
            print("   âš ï¸ [Warn] ë³¸ë¬¸ ì˜ì—­ ëª» ì°¾ìŒ. ì „ì²´ í…ìŠ¤íŠ¸ ìˆ˜ì§‘ìœ¼ë¡œ ëŒ€ì²´.")
            extracted_data["job_description"] = clean_space(soup.body.get_text())[:1000]

        return extracted_data

    except Exception as e:
        print(f"   âŒ [Detail] ì—ëŸ¬ ë°œìƒ: {e}")
        return None

def scrape_and_produce(page_index=1):
    producer = get_kafka_producer()
    topic_name = 'career_raw'
    
    print(f"ğŸš€ [Page {page_index}] í¬ë¡¤ë§ ì‹œì‘ (êµ¬ì¡°í™” ë°ì´í„° ì¶”ì¶œ)...")
    
    # ë¦¬ìŠ¤íŠ¸ í˜ì´ì§€ (ê³ ìš©24)
    url = f"https://www.work24.go.kr/wk/a/b/1200/retriveDtlEmpSrchList.do?occupation=135101%7C135102%7C136102%7C026%7C024&pageIndex={page_index}"
    headers = {
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
    }

    try:
        response = requests.get(url, headers=headers)
        soup = BeautifulSoup(response.text, 'html.parser')
        job_rows = soup.find_all('tr', id=re.compile(r'^list\d+'))
        
        print(f"ğŸ“Œ [List] ê³µê³  {len(job_rows)}ê°œ ë°œê²¬")
        
        count = 0
        for i, row in enumerate(job_rows):
            print(f"\n--- [ê³µê³  #{i+1}] ---")
            
            # K-Number
            row_html = str(row)
            auth_match = re.search(r"(K\d{10,})", row_html)
            if not auth_match: continue
            auth_no = auth_match.group(1)
            
            # ê¸°ë³¸ ì •ë³´ Parsing
            cols = row.select('td')
            if len(cols) < 3: continue
            
            td0_parts = cols[0].get_text(separator='|', strip=True).split('|')
            company = td0_parts[0].strip() if len(td0_parts) > 0 else "N/A"
            
            title = "N/A"
            if len(td0_parts) > 1:
                potential_title = td0_parts[1].strip()
                if "ì…ì‚¬ì§€ì›" in potential_title or "ìš”ì•½ë³´ê¸°" in potential_title:
                     if len(td0_parts) > 2: title = td0_parts[2].strip()
                else:
                    title = potential_title

            td1_parts = cols[1].get_text(separator='|', strip=True).split('|')
            pay, location = "ë©´ì ‘ í›„ ê²°ì •", "ì§€ì—­ ë¯¸ìƒ"
            for part in td1_parts:
                part = clean_space(part)
                if any(x in part for x in ["ì—°ë´‰", "ì›”ê¸‰", "ì‹œê¸‰"]): pay = part
                elif any(x in part for x in ["ì‹œ ", "êµ¬ ", "êµ° "]) and "ì£¼" not in part: location = part

            td2_text = cols[2].get_text(separator='|', strip=True)
            reg_match = re.search(r"ë“±ë¡ì¼\s?:\s?(\d{4}-\d{2}-\d{2})", td2_text)
            reg_date = reg_match.group(1) if reg_match else time.strftime('%Y-%m-%d')
            deadline_match = re.search(r"ë§ˆê°ì¼\s?:\s?(\d{4}-\d{2}-\d{2})", td2_text)
            deadline = deadline_match.group(1) if deadline_match else "ì±„ìš©ì‹œê¹Œì§€"

            # -----------------------------------------------------
            # [ìƒì„¸] êµ¬ì¡°í™”ëœ ë°ì´í„° ì¶”ì¶œ í˜¸ì¶œ
            # -----------------------------------------------------
            detail_data = get_detail_info(auth_no)
            
            if detail_data:
                job_desc = detail_data["job_description"]
                req = detail_data["requirements"]
                pref = detail_data["preferred"]
            else:
                job_desc = "ìˆ˜ì§‘ ì‹¤íŒ¨"
                req = "ìˆ˜ì§‘ ì‹¤íŒ¨"
                pref = "ìˆ˜ì§‘ ì‹¤íŒ¨"

            # ì›Œí¬ë„· ë§í¬ (ì‚¬ìš©ì ì œê³µìš©)
            worknet_link = f"https://www.work.go.kr/empInfo/empInfoSrch/detail/empDetailAuthView.do?wantedAuthNo={auth_no}"
            
            data = {
                "source_id": auth_no,
                "company": company,
                "title": title,
                "link": worknet_link,
                "pay": pay,
                "location": location,
                "deadline": deadline,
                "reg_date": reg_date,
                # â–¼ ê¹”ë”í•˜ê²Œ ë¶„ë¦¬ëœ í•„ë“œë“¤
                "description": job_desc,      # ìˆœìˆ˜ ì§ë¬´ë‚´ìš©
                "requirements": req,          # ìê²©ìš”ê±´ (ê²½ë ¥, í•™ë ¥ ë“±)
                "preferred_qualifications": pref, # ìš°ëŒ€ì‚¬í•­
                "collected_at": time.strftime('%Y-%m-%dT%H:%M:%SZ')
            }
            
            producer.send(topic_name, value=data)
            print("   âœ… [Kafka] ì „ì†¡ ì™„ë£Œ")
            count += 1
                
        producer.flush()
        print(f"\nğŸ‰ ì´ {count}ê±´ì˜ ë°ì´í„°ë¥¼ '{topic_name}' í† í”½ìœ¼ë¡œ ì „ì†¡í–ˆìŠµë‹ˆë‹¤.")

    except Exception as e:
        print(f"âŒ [Critical] ì—ëŸ¬: {e}")
    finally:
        producer.close()

if __name__ == "__main__":
    scrape_and_produce(page_index=1)