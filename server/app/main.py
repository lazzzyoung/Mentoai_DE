import os
import logging
from typing import List, Dict, Any, Optional
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel, Field
from dotenv import load_dotenv
import psycopg2
from psycopg2.extras import RealDictCursor
from langchain_google_genai import ChatGoogleGenerativeAI, HarmCategory, HarmBlockThreshold
from langchain_huggingface import HuggingFaceEmbeddings
from qdrant_client import QdrantClient
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser, JsonOutputParser
from langchain_core.documents import Document
from langchain_qdrant import QdrantVectorStore

load_dotenv()
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI(title="MentoAI RAG Server")

# --- ì„¤ì • ---
DB_URL = os.getenv("DATABASE_URL", "postgresql://airflow:airflow@postgres:5432/mentoai")
QDRANT_HOST = os.getenv("QDRANT_HOST", "mentoai-qdrant")
QDRANT_URL = f"http://{QDRANT_HOST}:6333"
COLLECTION_NAME = "career_jobs"

# --- AI ëª¨ë¸ ì´ˆê¸°í™” ---
logger.info("Loading Embedding Model...")
embeddings = HuggingFaceEmbeddings(
    model_name="BM-K/KoSimCSE-roberta-multitask",
    model_kwargs={'device': 'cpu'},
    encode_kwargs={'normalize_embeddings': True}
)

logger.info(f"ğŸ”Œ Connecting to Qdrant at {QDRANT_URL}...")
client = QdrantClient(url=QDRANT_URL)

# ê²€ìƒ‰ìš© Store
vector_store = QdrantVectorStore(
    client=client,
    collection_name=COLLECTION_NAME,
    embedding=embeddings,
    content_payload_key="full_text",
    metadata_payload_key=None
)

logger.info("ğŸ§  Initializing Google Gemini 3 Flash Preview...")
llm = ChatGoogleGenerativeAI(
    model="gemini-3-flash-preview", 
    google_api_key=os.getenv("GOOGLE_API_KEY"),
    temperature=0.3, 
    safety_settings={
        HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,
        HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,
        HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,
        HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,
    },
)

def fetch_user_info(user_id: int):
    conn = None
    try:
        conn = psycopg2.connect(DB_URL)
        cur = conn.cursor(cursor_factory=RealDictCursor)
        cur.execute("SELECT u.username, s.desired_job, s.career_years, s.skills FROM user_specs s JOIN users u ON s.user_id = u.id WHERE s.user_id = %s", (user_id,))
        user_info = cur.fetchone()
        if not user_info: raise HTTPException(404, "User not found")
        return user_info
    finally:
        if conn: conn.close()


class UserSpecResponse(BaseModel):
    user_id: int
    desired_job: str
    career_years: int
    education: str
    skills: List[str]
    certificates: List[str]

# [V1 ëª¨ë¸]
class RoadmapResponseV1(BaseModel):
    user_name: str
    recommended_jobs: List[str]
    analysis_result: str

# [V2 ëª¨ë¸]
class JobRecommendation(BaseModel):
    id: int
    company: str
    title: str

class GapItem(BaseModel):
    skill: str
    score_impact: int
    action_guide: str

class RoadmapStep(BaseModel):
    step_name: str
    description: str

class AnalysisResultV2(BaseModel):
    current_score: int
    summary: str
    gap_analysis: List[GapItem]
    roadmap: List[RoadmapStep]

class RoadmapResponseV2(BaseModel):
    user_name: str
    recommended_jobs: List[JobRecommendation]
    analysis_result: AnalysisResultV2

# [V3 ëª¨ë¸] - ëª©ë¡ ì¡°íšŒìš©
class JobSummary(BaseModel):
    job_id: int
    company: str
    title: str
    match_score: int = Field(description="ì í•©ë„ ì ìˆ˜ (60~100)")
    max_score: int = Field(default=100, description="ë§Œì  ê¸°ì¤€")
    reason: str = Field(description="ì¶”ì²œ ì´ìœ  í•œ ì¤„ ìš”ì•½")

class JobSummaryList(BaseModel):
    jobs: List[JobSummary]

class RecommendationListResponse(BaseModel):
    user_name: str
    recommendations: List[JobSummary]

# [V3 ëª¨ë¸] - ìƒì„¸ ì¡°íšŒìš©
class ActionItem(BaseModel):
    category: str
    item_name: str
    description: str
    expected_score_up: int

class DetailedAnalysisResponse(BaseModel):
    job_title: str
    company_name: str
    current_score: int
    max_score: int = 100
    analysis_summary: str
    required_tech_stack: List[str]
    action_plan: List[ActionItem]
    interview_tip: str


# =========================================================
# ê¸°ë³¸ ì—”ë“œí¬ì¸íŠ¸
# =========================================================

@app.get("/")
def health_check():
    return {"status": "ok", "message": "MentoAI Brain is running with Gemini"}

@app.get("/api/v1/test/gemini")
def test_gemini_connection(prompt: str = "ì•ˆë…•"):
    try:
        response = llm.invoke(prompt)
        return {"gemini_response": response.content}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/v1/users/{user_id}/specs", response_model=UserSpecResponse)
def get_user_specs(user_id: int):
    conn = None
    try:
        conn = psycopg2.connect(DB_URL)
        cur = conn.cursor(cursor_factory=RealDictCursor)
        cur.execute("SELECT * FROM user_specs WHERE user_id = %s", (user_id,))
        result = cur.fetchone()
        if not result: raise HTTPException(404, "User not found")
        return result
    finally:
        if conn: conn.close()


# =========================================================
# [V1] ê¸°ì¡´ Markdown ë¡œë“œë§µ
# =========================================================
@app.post("/api/v1/curation/roadmap/{user_id}", response_model=RoadmapResponseV1)
def generate_career_roadmap_v1(user_id: int):
    try:
        user_info = fetch_user_info(user_id)
        user_query_text = f"í¬ë§ì§ë¬´: {user_info['desired_job']}, ë³´ìœ ê¸°ìˆ : {', '.join(user_info['skills'] or [])}"
        
        retrieved_docs = vector_store.similarity_search(user_query_text, k=3)
        if not retrieved_docs:
             return RoadmapResponseV1(user_name=user_info['username'], recommended_jobs=[], analysis_result="ê³µê³  ì—†ìŒ")

        template = """
        [ì‚¬ìš©ì] {user_specs}
        [ì±„ìš©ê³µê³ ] {context}
        í•©ê²©ì„ ìœ„í•œ ì „ëµì  ë¡œë“œë§µì„ Markdownìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.
        """
        prompt = ChatPromptTemplate.from_template(template)
        formatted_context = "\n".join([f"ê¸°ì—…: {d.metadata.get('company')}\nì œëª©: {d.metadata.get('position')}\në‚´ìš©: {d.page_content[:300]}" for d in retrieved_docs])
        
        chain = prompt | llm | StrOutputParser()
        analysis_result = chain.invoke({"user_specs": user_query_text, "context": formatted_context})

        return RoadmapResponseV1(
            user_name=user_info['username'],
            recommended_jobs=[doc.metadata.get('position') or "ë¯¸ìƒ" for doc in retrieved_docs],
            analysis_result=analysis_result
        )
    except Exception as e:
        logger.error(f"V1 Error: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))


# =========================================================
# [V2] JSON êµ¬ì¡°í™” ë¡œë“œë§µ
# =========================================================
@app.post("/api/v2/curation/roadmap/{user_id}", response_model=RoadmapResponseV2)
def generate_career_roadmap_v2(user_id: int):
    try:
        user_info = fetch_user_info(user_id)
        user_query_text = f"í¬ë§ì§ë¬´: {user_info['desired_job']}, ë³´ìœ ê¸°ìˆ : {', '.join(user_info['skills'] or [])}"
        
        retrieved_docs = vector_store.similarity_search(user_query_text, k=3)
        if not retrieved_docs:
             empty = AnalysisResultV2(current_score=0, summary="ê³µê³  ì—†ìŒ", gap_analysis=[], roadmap=[])
             return RoadmapResponseV2(user_name=user_info['username'], recommended_jobs=[], analysis_result=empty)

        parser = JsonOutputParser(pydantic_object=AnalysisResultV2)
        template = """
        ì‚¬ìš©ì ìŠ¤í™ê³¼ ê³µê³ ë¥¼ ë¹„êµí•˜ì—¬ JSONìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”. ë§ˆí¬ë‹¤ìš´ ì‚¬ìš© ê¸ˆì§€.
        [í”„ë¡œí•„] {user_specs}
        [ê³µê³ ] {context}
        {format_instructions}
        """
        prompt = ChatPromptTemplate.from_template(template)
        formatted_context = "\n".join([f"ê¸°ì—…: {d.metadata.get('company')}\nì œëª©: {d.metadata.get('position')}\në‚´ìš©: {d.page_content[:500]}" for d in retrieved_docs])
        
        chain = prompt | llm | parser
        analysis_result_dict = chain.invoke({
            "user_specs": user_query_text, 
            "context": formatted_context,
            "format_instructions": parser.get_format_instructions()
        })
        
        recommended_jobs = [
            JobRecommendation(
                id=d.metadata.get('id') or d.metadata.get('_id') or 0,
                company=d.metadata.get('company') or "ë¯¸ìƒ",
                title=d.metadata.get('position') or "ë¯¸ìƒ"
            ) for d in retrieved_docs
        ]

        return RoadmapResponseV2(
            user_name=user_info['username'],
            recommended_jobs=recommended_jobs,
            analysis_result=analysis_result_dict
        )
    except Exception as e:
        logger.error(f"V2 Error: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))


# =========================================================
# [V3 - API 1] ê¸°ì—… ëª©ë¡ ë° ì ìˆ˜ ì¡°íšŒ
# =========================================================
@app.post("/api/v3/jobs/recommend/{user_id}", response_model=RecommendationListResponse)
def recommend_jobs_list(user_id: int):
    try:
        user_info = fetch_user_info(user_id)
        user_query_text = f"í¬ë§ì§ë¬´: {user_info['desired_job']}, ë³´ìœ ê¸°ìˆ : {', '.join(user_info['skills'] or [])}, ê²½ë ¥: {user_info['career_years']}ë…„"
        
        retrieved_docs = vector_store.similarity_search(user_query_text, k=5)
        
        if not retrieved_docs:
            return RecommendationListResponse(user_name=user_info['username'], recommendations=[])

        jobs_context = []
        for doc in retrieved_docs:
            doc_id = doc.metadata.get('id') or doc.metadata.get('_id')
            
            # ê¸°ë³¸ê°’ ì„¤ì •
            company = "ë¯¸ìƒ"
            title = "ë¯¸ìƒ"
            content = doc.page_content

            if doc_id:
                try:
                    
                    points = client.retrieve(
                        collection_name=COLLECTION_NAME,
                        ids=[doc_id],
                        with_payload=True
                    )
                    if points:
                        payload = points[0].payload
                        company = payload.get('company', "ë¯¸ìƒ")
                        title = payload.get('position', "ë¯¸ìƒ")
                        content = payload.get('full_text', content)
                except Exception as e:
                    logger.warning(f"Metadata fetch failed for ID {doc_id}: {e}")

            jobs_context.append({
                "job_id": doc_id,
                "company": company,
                "title": title,
                "content": content[:300] 
            })

        # LLM ì±„ì 
        parser = JsonOutputParser(pydantic_object=JobSummaryList)
        
        template = """
        ë‹¹ì‹ ì€ ì•„ì£¼ ê¹ê¹í•˜ê³  ì—„ê²©í•œ IT ë©´ì ‘ê´€ì…ë‹ˆë‹¤.
        [ì‚¬ìš©ì í”„ë¡œí•„]ê³¼ [ê³µê³  ëª©ë¡]ì„ ë¹„êµí•˜ì—¬ ëƒ‰ì •í•˜ê²Œ ì í•©ë„ ì ìˆ˜ë¥¼ ë§¤ê¸°ì„¸ìš”.
        
        [ì‚¬ìš©ì í”„ë¡œí•„] {user_specs}
        [ê³µê³  ëª©ë¡] {jobs_context}
        
        **ì±„ì  ê¸°ì¤€ (Strict Scoring):**
        1. **ê¸°ë³¸ ì ìˆ˜ëŠ” 50ì **ì—ì„œ ì‹œì‘í•˜ì„¸ìš”.
        2. **ê°ì  ìš”ì¸**: 
           - ê³µê³ ê°€ 'ì‹œë‹ˆì–´(4ë…„ ì´ìƒ)'ë¥¼ ìš”êµ¬í•˜ëŠ”ë° ì‚¬ìš©ìê°€ 'ì‹ ì…/ì£¼ë‹ˆì–´'ë¼ë©´ **ë¬´ì¡°ê±´ 70ì  ë¯¸ë§Œ**ìœ¼ë¡œ ì±„ì í•˜ì„¸ìš”.
           - í´ë¼ìš°ë“œ(AWS/GCP), Kubernetes, ìš´ì˜ ê²½í—˜ ë“± í•µì‹¬ ì—­ëŸ‰ì´ ë¶€ì¡±í•˜ë©´ ê°€ì°¨ ì—†ì´ ê°ì í•˜ì„¸ìš”.
        3. **ê°€ì‚° ìš”ì¸**: ê¸°ìˆ  ìŠ¤íƒ(Spark, Kafka ë“±)ì´ ì •í™•íˆ ì¼ì¹˜í•  ë•Œë§Œ ì ìˆ˜ë¥¼ ì˜¬ë¦¬ì„¸ìš”.
        4. **ìµœì¢… ì ìˆ˜**: ë³´í†µ 60~85ì  ì‚¬ì´ê°€ ë‚˜ì™€ì•¼ ì •ìƒì…ë‹ˆë‹¤. 90ì  ì´ìƒì€ ì™„ë²½í•˜ê²Œ ì¼ì¹˜í•  ë•Œë§Œ ì£¼ì„¸ìš”.
        5. match_score, reason, job_id, company, title í•„ë“œë¥¼ í¬í•¨í•˜ì—¬ JSONìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”.
        
        **ì¶œë ¥ í¬ë§· (JSON):**
        {format_instructions}
        """
        
        prompt = ChatPromptTemplate.from_template(template)
        chain = prompt | llm | parser
        
        # ê²°ê³¼ íŒŒì‹±
        result = chain.invoke({
            "user_specs": user_query_text,
            "jobs_context": str(jobs_context),
            "format_instructions": parser.get_format_instructions()
        })
        
        scored_jobs = result.get("jobs", [])
        
        return RecommendationListResponse(
            user_name=user_info['username'],
            recommendations=scored_jobs
        )

    except Exception as e:
        logger.error(f"V3 List Error: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

# =========================================================
# [V3 - API 2] ìƒì„¸ ì»¨ì„¤íŒ…
# =========================================================
@app.post("/api/v3/jobs/{job_id}/analyze/{user_id}", response_model=DetailedAnalysisResponse)
def analyze_job_detail(job_id: int, user_id: int):
    try:
        user_info = fetch_user_info(user_id)
        user_query_text = f"í¬ë§ì§ë¬´: {user_info['desired_job']}, ë³´ìœ ê¸°ìˆ : {', '.join(user_info['skills'] or [])}"
        
        # ìƒì„¸ ì¡°íšŒ
        points = client.retrieve(
            collection_name=COLLECTION_NAME,
            ids=[job_id],
            with_payload=True
        )
        
        if not points:
            raise HTTPException(404, "í•´ë‹¹ ê³µê³ ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            
        job_payload = points[0].payload
        job_full_text = job_payload.get('full_text', '')
        company = job_payload.get('company', "ë¯¸ìƒ")
        title = job_payload.get('position', "ë¯¸ìƒ")
        
        # LLM ìƒì„¸ ì»¨ì„¤íŒ…
        parser = JsonOutputParser(pydantic_object=DetailedAnalysisResponse)
        
        template = """
        ë‹¹ì‹ ì€ IT ëŒ€ê¸°ì—… ë° ìœ ë‹ˆì½˜ ìŠ¤íƒ€íŠ¸ì—…ì˜ **ì‹œë‹ˆì–´ í…Œí¬ ë¦¬ë“œ(Tech Lead)**ì´ì ì±„ìš© ìµœì¢… ê²°ì •ê¶Œìì…ë‹ˆë‹¤.
        ì§€ì›ìì˜ ì´ë ¥ì„œì™€ ê³µê³ ë¥¼ ë¹„êµ ë¶„ì„í•˜ì—¬, ë‹¹ì¥ ì‹¤ì²œ ê°€ëŠ¥í•œ **'í•©ê²© ì¹˜íŠ¸í‚¤'** ìˆ˜ì¤€ì˜ ì „ëµì„ ìˆ˜ë¦½í•˜ì„¸ìš”.
        
        [ì§€ì›ì í”„ë¡œí•„] {user_specs}
        [ëª©í‘œ ê³µê³ ] {company} / {title} / {content}
        
        **ì‘ì„± ì§€ì¹¨ (Deep Dive):**
        
        1. **current_score (ëƒ‰ì² í•œ í‰ê°€)**: 
           - 50~85ì  ì‚¬ì´ë¡œ ì±…ì •í•˜ë˜, 'ì™œ ê°ì ë˜ì—ˆëŠ”ì§€'ë¥¼ ë¶„ì„í•˜ì—¬ ì•„ë˜ ì•¡ì…˜ í”Œëœì— ë…¹ì—¬ë‚´ì„¸ìš”.
           
        2. **required_tech_stack (í•µì‹¬ íŒŒì•…)**: 
           - ê³µê³ ì— ë‚˜ì—´ëœ ê¸°ìˆ  ì¤‘, ì§€ì›ìê°€ ì—†ìœ¼ë©´ ì„œë¥˜ ê´‘íƒˆí•  ë§Œí•œ **Critical Stack** 3~5ê°€ì§€ë§Œ ì—„ì„ í•˜ì„¸ìš”.
           
        3. **action_plan (ì´ˆêµ¬ì²´ì  ì‹¤í–‰ ê°€ì´ë“œ)**: 
           - ì¶”ìƒì ì¸ ì¡°ì–¸(ì˜ˆ: "Kubernetes ê³µë¶€í•˜ê¸°")ì€ **ì ˆëŒ€ ê¸ˆì§€**ì…ë‹ˆë‹¤.
           - **How-toë¥¼ í¬í•¨í•œ ì‹œë‚˜ë¦¬ì˜¤**ë¥¼ ì œì‹œí•˜ì„¸ìš”.
           - **ì˜ˆì‹œ**:
             - (Bad) "í´ë¼ìš°ë“œ ê³µë¶€í•˜ì„¸ìš”."
             - (Good) "í˜„ì¬ ë³´ìœ í•œ FastAPI í”„ë¡œì íŠ¸ë¥¼ Docker ì´ë¯¸ì§€ë¡œ ë¹Œë“œí•˜ê³ , **AWS EKS(Free Tier)**ì— ë°°í¬í•˜ëŠ” ì‹¤ìŠµì„ í•˜ì„¸ìš”. ì´ë•Œ **Terraform**ìœ¼ë¡œ ì¸í”„ë¼ë¥¼ í”„ë¡œë¹„ì €ë‹í•˜ì—¬ 'IaC ê²½í—˜'ì„ í¬íŠ¸í´ë¦¬ì˜¤ì— í•œ ì¤„ ì¶”ê°€í•´ì•¼ í•©ë‹ˆë‹¤."
             - (Good) "ì§€ì›ìëŠ” Spark ê²½í—˜ì´ ìˆìœ¼ë‹ˆ, **Airflow**ì™€ ì—°ë™í•˜ì—¬ 'ë§¤ì¼ 09ì‹œì— S3 ë°ì´í„°ë¥¼ ê¸ì–´ì™€ ë§ˆíŠ¸ë¥¼ ìƒì„±í•˜ëŠ” DAG'ë¥¼ êµ¬í˜„í•˜ê³  ê¹ƒí—ˆë¸Œì— ì˜¬ë¦¬ì„¸ìš”."
             
        4. **interview_tip (ë©´ì ‘ê´€ì˜ ì‹œì„ )**: 
           - í•´ë‹¹ íšŒì‚¬ì˜ ë„ë©”ì¸(í•€í…Œí¬, ì»¤ë¨¸ìŠ¤, AI ë“±)ê³¼ ê¸°ìˆ  ìŠ¤íƒì„ ê²°í•©í•œ **ì˜ˆìƒ ì§ˆë¬¸**ì„ ë˜ì§€ê³ , **ëª¨ë²” ë‹µì•ˆì˜ í‚¤ì›Œë“œ**ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”.
        
        **ì¶œë ¥ í¬ë§· (JSON):**
        {format_instructions}
        """
        
        prompt = ChatPromptTemplate.from_template(template)
        chain = prompt | llm | parser
        
        analysis_result = chain.invoke({
            "user_specs": user_query_text,
            "company": company,
            "title": title,
            "content": job_full_text,
            "format_instructions": parser.get_format_instructions()
        })
        
        # ë©”íƒ€ë°ì´í„° ë³´ì •
        analysis_result['job_title'] = title
        analysis_result['company_name'] = company
        
        return analysis_result

    except Exception as e:
        logger.error(f"V3 Detail Error: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))